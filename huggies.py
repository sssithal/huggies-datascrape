# -*- coding: utf-8 -*-
"""huggies.ipynb

Automatically generated by Colab.

# installations
!pip install -q langchain openai tiktoken faiss-cpu

"""# create embeddings"""

## part 1 - scrape huggies website

# create a webscraper function

from bs4 import BeautifulSoup

def extract_text_from(url):
    html = requests.get(url).text
    soup = BeautifulSoup(html, features="html.parser")
    text = soup.get_text()

    lines = (line.strip() for line in text.splitlines())
    return '\n'.join(line for line in lines if line)

#obtain all urls associated with huggies

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

def get_all_urls(base_url):
    urls = set()
    response = requests.get(base_url)

    if response.status_code == 200:
        soup = BeautifulSoup(response.content, 'html.parser')
        anchor_tags = soup.find_all('a', href=True)

        for anchor in anchor_tags:
            href = anchor['href']
            full_url = urljoin(base_url, href)
            if full_url.startswith('https://www.huggies.com/en-us/'):
              urls.add(full_url)
    return list(urls)

# extract all text from all urls
base_url = 'https://www.huggies.com'
all_urls = [base_url] + get_all_urls(base_url)
pages = [{'text': extract_text_from(url), 'source': url} for url in all_urls]

# store it as a json lines file
import json

with open("huggies_full_website.jsonl", 'w') as file:
    for page in pages:
        file.write(json.dumps(page) + '\n')

with open("huggies_full_website.jsonl", 'r') as file:
    pages = [json.loads(line) for line in file]

# split and create metadata

from langchain.text_splitter import CharacterTextSplitter

# print(pages)
#for page in pages:


text_splitter = CharacterTextSplitter(chunk_size=4000, chunk_overlap = 100, separator="\n")
docs, metadatas = [], []
for page in pages:
    splits = text_splitter.split_text(page['text'])
    docs.extend(splits)

    metadatas.extend([{"source": page['source']}] * len(splits))
    #print("Scraped Text: ", page['text'])
    print(f"Website: Split {page['source']} into {len(splits)} chunks")
    print("\n")

import faiss
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings
import pickle
import openai

#print(docs)

store = FAISS.from_texts(docs, OpenAIEmbeddings(openai_api_key = "{Insert Own Key}"), metadatas=metadatas)


with open("KC_HUGGIES_faiss.pkl", "wb") as f:
    pickle.dump(store, f)

from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.document_loaders import TextLoader
import requests
from bs4 import BeautifulSoup
from langchain.chains import ChatVectorDBChain
from langchain.chat_models import ChatOpenAI
import re
import pickle

# Define your OpenAI API key here
openai_api_key = "{Insert Own Key}"

# Initialize global variables
chat_history = []
db = None

# Initialize Langchain objects
llm = ChatOpenAI(
    openai_api_key=openai_api_key,
    model_name='gpt-3.5-turbo',
    temperature=0.0
)

def query_from_doc(text):
    global chat_history
    qa = ChatVectorDBChain.from_llm(llm, db)
    # Query the Langchain ChatVectorDBChain with a user's input text.
    # Prompt template for the chat
    prompt_template = (
        "You are an expert at customer interactions and will answer the given question only "
        "if you know the answer, or else you will reply with 'Please check Huggies website for more information.'"
    )

    result = qa({"question": prompt_template + text, "chat_history": chat_history})
    # Update chat history
    chat_history = [(text, result["answer"])]
    return result["answer"]

def query_faiss(query):
    # Query the FAISS vector database with a user's query.
    global db
    ans = db.similarity_search(query)
    return ans[0].page_content

def load_faiss_local(path):
    global db
    with open(path, "rb") as f:
        db = pickle.load(f)
    print("db loaded")

def query_with_link(query):
    # Combine GPT-3.5 Turbo model's response and relevant links from the FAISS database.
    new_db = db.similarity_search(query)

    # Collect unique relevant links
    relevant_links = list(set(i.metadata['source'] for i in new_db))

    # Construct a formatted link string
    links = '\n'.join(relevant_links)

    # Get the response from the GPT-3.5 Turbo model
    response_from_chatgpt = query_from_doc(query)

    # Create the final response with links
    final_response = (
        response_from_chatgpt +
        "\n\nHere are some of the relevant links from The Huggies Website:\n" +
        links
    )

    return final_response

def query_with_link_alt(query):

    # Perform a similarity search and combine text from the FAISS results.

    new_db = db.similarity_search(query)
    text_for_llm = ''

    # Combine text content and source information
    for i in new_db:
        text_for_llm += i.page_content + ' (Source of text: ' + i.metadata['source'] + ')'

load_faiss_local("/content/KC_HUGGIES_faiss.pkl")

query_with_link("Where do I find out about Huggies in a different country?")

# Define an introductory message
intro_message = "I'd like to learn more about Huggies. Can you provide some key details?"

# Send the introductory message
response1 = query_from_doc(intro_message)

# Follow up with specific concise questions
query1 = "What are Huggies products?"
query2 = "Tell me about Huggies history."

# Send the queries one by one
response2 = query_from_doc(query1)
response3 = query_from_doc(query2)

# Combine and display the responses
combined_response = (
    intro_message + "\n\nResponse 1: " + response1 + "\n\nResponse 2: " + response2 + "\n\nResponse 3: " + response3
)
print(combined_response)

soup = BeautifulSoup(html, "html.parser")

# using beautifulsoup to locate links
all_links = soup.find_all("a")

scraped_links_info = []

# extract specific information from each element
for link in all_links:
    link_text = link.get_text()
    link_url = link.get("href")
    if link_url and (link_url.startswith("http") or link_url.startswith("https")):
      print("Scraped_text:", link_text)
      print("Website:", link_url )
      scraped_links_info.append({"link_text": link_text, "link_url": link_url})

all_embeddings = []

for link_info in scraped_links_info:
    link_text = link_info["link_text"]
    link_url = link_info["link_url"]

    # recursive text splitter - splits text into paragraphs
    text_splitter = RecursiveCharacterTextSplitter(
    chunk_size = 1500,
    chunk_overlap = 150
    )

    # processing text into smaller chunks
    chunks = [link_text[i:i+1500] for i in range(0, len(link_text), 1500 - 150)]


    # process and embed each paragraph
    embeddings = []


    for chunk in chunks:
        # generate embeddings using OpenAI's API
        response = openai.Embedding.create(model= "text-embedding-ada-002",input=chunk)
        embedding_data = response["data"][0]["embedding"]
        embeddings.append(embedding_data)
        # print(response)
    all_embeddings.append(embeddings)
